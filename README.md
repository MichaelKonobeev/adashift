# Adashift

Reproducing of the [AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods](https://openreview.net/forum?id=HkgTkhRcKQ)


## Experiments

## Results

**Synthetic Experiment**

![Synthetic](/img/regret_synth.png) ![Synthetic_optval](/img/opt_synth.png)


**Logistic Regression on MNIST**

![LR1](/img/mnist_LR_smooth_1000.png) ![LR2](/img/mnist_LR_1000.png)

**W-GAN**

![wgan](/img/wgan_train_loss100.png)

## Dependencies

- Python 3.4+,
- NumPy 
- PyTorch 0.4+
- SciPy, Matplotlib
- GPU for Dl experiments
### References

[On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=piqcy)

[ Adam ](https://arxiv.org/abs/1412.6980)


[AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods](https://arxiv.org/abs/1810.00143)

